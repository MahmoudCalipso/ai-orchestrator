# Local AI Models Configuration
# Use open-source models without API keys

# Default LLM Provider (use 'ollama' for local models)
LLM_PROVIDER=ollama  # Options: openai, anthropic, ollama, azure, vllm

# Ollama Configuration (Local Models)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5-coder:32b  # Default model for coding tasks

# ========================================
# 2026 STATE-OF-THE-ART MODELS
# ========================================

# TIER 1: THE "BRAIN" - High-Level Orchestration & Architecture
# Use for: Lead Architect Agent, Complex Decision Making, Verification
# --------------------------------------------------------------------
# qwen3:235b - #1 Open-Source Coder (Alibaba) - 256K context
# llama4:100b - Gold Standard for Stability (Meta) - 1M+ context
# deepseek-v3.2:671b - Most Cost-Efficient (37B active MoE)

# TIER 2: THE "REASONER" - Deep Analysis & Planning
# Use for: Deep Scan, Audit Agent, Legacy Code Analysis
# --------------------------------------------------------------------
# deepseek-r1:70b - Chain-of-Thought reasoning for complex analysis
# deepseek-r1:32b - Faster reasoning for medium complexity
# qwen2.5-coder:32b - Dense coding knowledge, excellent for analysis

# TIER 3: THE "WORKERS" - Fast Code Generation
# Use for: Engineer Agents, Bulk Code Writing, Repetitive Tasks
# --------------------------------------------------------------------
# qwen2.5-coder:14b - Fast, high-quality code generation
# qwen2.5-coder:7b - Fastest, good for simple tasks
# glm-4.6:9b - Frontend/UI specialist (Figma to code)
# codellama:13b - Reliable fallback for general coding

# Memory Optimization
OLLAMA_NUM_GPU=1  # Number of GPUs to use (0 for CPU only)
OLLAMA_NUM_THREAD=8  # CPU threads
OLLAMA_NUM_CTX=4096  # Context window size (reduce for less memory)
OLLAMA_LOW_VRAM=true  # Enable for systems with limited VRAM

# Model Download Directory
OLLAMA_MODELS_DIR=./models/ollama

# Optional: OpenAI (if you want to use it as fallback)
# OPENAI_API_KEY=your-key-here

# Optional: Anthropic (if you want to use it as fallback)
# ANTHROPIC_API_KEY=your-key-here

# Git Configuration
GITHUB_TOKEN=
GITLAB_TOKEN=
BITBUCKET_APP_PASSWORD=
GIT_ENCRYPTION_KEY=

# Security
API_KEY=dev-key-12345  # Change this in production

# Database
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/orchestrator
REDIS_URL=redis://localhost:6379

# Logging
LOG_LEVEL=INFO
