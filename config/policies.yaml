policies:
  routing:
    # Task-based routing
    by_task_type:
      code_generation:
        models: [qwen2.5-coder:7b, deepseek-coder:6.7b, codellama:7b]
        min_context: 8192
        temperature: 0.2
        top_p: 0.95
        
      code_review:
        models: [deepseek-coder:6.7b, qwen2.5-coder:7b]
        min_context: 16384
        temperature: 0.1
        top_p: 0.9
        
      reasoning:
        models: [qwen2.5:14b, mistral:7b, phi3:3.8b]
        min_context: 8192
        temperature: 0.3
        top_p: 0.9
        
      quick_query:
        models: [mistral:7b, phi3:3.8b]
        max_context: 4096
        temperature: 0.7
        top_p: 0.95
        timeout: 30
        
      creative_writing:
        models: [mistral:7b, qwen2.5:14b]
        temperature: 0.8
        top_p: 0.95
        top_k: 50
        
      data_analysis:
        models: [qwen2.5:14b, mistral:7b]
        min_context: 16384
        temperature: 0.2
        
      documentation:
        models: [mistral:7b, qwen2.5-coder:7b]
        temperature: 0.3
        top_p: 0.9
        
      chat:
        models: [mistral:7b, phi3:3.8b, codellama:7b]
        temperature: 0.7
        max_tokens: 1024
        
    # Complexity-based routing
    by_complexity:
      simple:
        models: [phi3:3.8b, mistral:7b]
        max_processing_time: 30
        
      moderate:
        models: [mistral:7b, codellama:7b]
        max_processing_time: 120
        
      complex:
        models: [qwen2.5-coder:7b, deepseek-coder:6.7b, qwen2.5:14b]
        max_processing_time: 300
        
      expert:
        models: [qwen2.5:14b, deepseek-coder:6.7b]
        max_processing_time: 600
        allow_chain_of_thought: true

  # Load Balancing
  load_balancing:
    strategy: weighted_round_robin
    weights:
      by_performance:
        vllm: 4
        ollama: 3
        transformers: 2
        llamacpp: 1
        
    circuit_breaker:
      enabled: true
      failure_threshold: 5
      timeout_duration: 60
      half_open_max_requests: 3
      
    rate_limiting:
      enabled: true
      requests_per_minute: 100
      burst_size: 20
      by_user: true
      
    queue_management:
      max_queue_size: 50
      queue_timeout: 300
      priority_levels: 3

  # Fallback Strategy
  fallback:
    enabled: true
    max_retries: 3
    retry_delay: 5  # seconds
    exponential_backoff: true
    
    cascade_order:
      - primary_model
      - secondary_model_same_family
      - smaller_model_same_capability
      - general_purpose_model
      
    failure_conditions:
      - timeout
      - out_of_memory
      - runtime_error
      - model_not_loaded
      - context_length_exceeded
      
    degradation_modes:
      timeout:
        action: switch_to_faster_model
        models: [phi3, mistral]
        
      out_of_memory:
        action: switch_to_smaller_model
        quantization: higher
        
      context_length_exceeded:
        action: truncate_or_switch
        truncation_strategy: sliding_window
        alternative_models: [llama3.1, qwen2.5]

  # Resource Management
  resource_management:
    cpu:
      max_usage_percent: 80
      min_available_percent: 20
      thread_allocation:
        high_priority: 8
        normal_priority: 4
        low_priority: 2
        
    gpu:
      max_usage_percent: 95
      min_available_memory: 2gb
      allocation_strategy: best_fit
      preemption_enabled: true
      priority_preemption: true
      
    memory:
      max_usage_percent: 85
      swap_threshold: 90
      oom_handling: graceful_degradation
      
    disk:
      cache_size: 100gb
      model_cache_ttl: 3600  # seconds
      clear_on_threshold: 90

  # Quality Assurance
  quality_assurance:
    validation:
      enabled: true
      check_output_format: true
      check_response_length: true
      check_toxicity: true
      check_coherence: true
      
    confidence_threshold:
      minimum: 0.7
      request_human_review: 0.5
      
    output_filtering:
      enabled: true
      remove_pii: true
      sanitize_code: true
      check_hallucinations: true
      
    monitoring:
      log_all_requests: true
      log_errors: true
      log_performance_metrics: true
      alert_on_degradation: true

  # Security Policies
  security:
    authentication:
      required: true
      methods: [api_key, oauth2, jwt]
      
    authorization:
      role_based: true
      roles:
        admin:
          can: [create, read, update, delete, configure]
        developer:
          can: [create, read, update]
        user:
          can: [create, read]
          
    input_validation:
      max_input_length: 32768
      sanitize_input: true
      block_injection_attempts: true
      
    output_sanitization:
      enabled: true
      remove_sensitive_data: true
      watermark_outputs: false
      
    audit:
      enabled: true
      log_all_access: true
      retention_days: 90

  # Cost Optimization
  cost_optimization:
    prefer_smaller_models: true
    quantization_preference: q4_k_m
    
    cost_tiers:
      free:
        models: [phi3:3.8b, mistral:7b]
        max_tokens: 2048
        rate_limit: 10
        
      standard:
        models: [codellama:7b, qwen2.5-coder:7b, deepseek-coder:6.7b]
        max_tokens: 8192
        rate_limit: 50
        
      premium:
        models: [qwen2.5:14b, deepseek-coder:6.7b]
        max_tokens: 32768
        rate_limit: 100
        
    auto_scaling:
      enabled: true
      scale_down_delay: 300
      min_idle_instances: 1
      max_instances: 5