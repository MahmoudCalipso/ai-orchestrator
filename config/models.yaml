models:
  # Code Generation Models
  deepseek-v3:
    family: deepseek
    size: 671b
    context_length: 128000
    capabilities: [code, reasoning, bilingual]
    specialization: full_project_generation
    memory_requirement: 160gb
    recommended_runtime: [vllm]
    
  # 2026 State-of-the-Art Models
  deepseek-v3.2:
    family: deepseek
    size: "671B (MoE)"
    capabilities: [coding, reasoning, multihop, precision]
    context_window: 128000
    memory_requirement: "40GB+ (8-bit quantization)"
    recommended_runtime: vllm
  
  llama-4-scout:
    family: llama
    size: "109B (MoE)"
    capabilities: [coding, tool-use, vision, fast-inference]
    context_window: 64000
    memory_requirement: "24GB"
    recommended_runtime: ollama
    
  llama-4-maverick:
    family: llama
    size: "400B (MoE)"
    capabilities: [reasoning, coding, high-fidelity]
    context_window: 128000
    memory_requirement: "80GB+"
    recommended_runtime: vllm

  qwen2.5-coder-32b:
    family: qwen
    size: "32B"
    capabilities: [coding, repair, optimization]
    context_window: 32000
    memory_requirement: "16GB"
    recommended_runtime: ollama
    
  deepseek-coder:
    family: deepseek
    size: 33b
    context_length: 16384
    quantization: [q4_k_m, q5_k_m, q8_0]
    capabilities: [code, reasoning]
    specialization: programming
    memory_requirement: 24gb
    recommended_runtime: [vllm, ollama]
    
  qwen2.5-coder:
    family: qwen
    size: 32b
    context_length: 32768
    capabilities: [code, instruct, multi_lang]
    specialization: library_specific_code
    memory_requirement: 20gb
    recommended_runtime: [vllm, ollama]

  codellama:
    family: llama
    size: 34b
    context_length: 16384
    quantization: [q4_k_m, q5_k_m]
    capabilities: [code, instruct]
    specialization: code_completion
    memory_requirement: 22gb
    recommended_runtime: [ollama, llamacpp]

  # Reasoning Models
  qwen2.5:
    family: qwen
    size: 72b
    context_length: 32768
    quantization: [q4_k_m, q5_k_m, q6_k]
    capabilities: [reasoning, math, code]
    specialization: complex_reasoning
    memory_requirement: 48gb
    recommended_runtime: [vllm, transformers]
    
  llama3.1:
    family: llama
    size: 70b
    context_length: 128000
    quantization: [q4_k_m, q5_k_m]
    capabilities: [general, reasoning, long_context]
    specialization: general_purpose
    memory_requirement: 45gb
    recommended_runtime: [vllm, ollama]

  # Fast Response Models
  mistral:
    family: mistral
    size: 7b
    context_length: 32768
    quantization: [q4_k_m, q5_k_m, q8_0]
    capabilities: [general, fast]
    specialization: quick_tasks
    memory_requirement: 6gb
    recommended_runtime: [ollama, llamacpp, vllm]
    
  phi3:
    family: phi
    size: 3.8b
    context_length: 128000
    quantization: [q4_k_m, q5_k_m]
    capabilities: [general, efficient, long_context]
    specialization: resource_efficient
    memory_requirement: 4gb
    recommended_runtime: [ollama, llamacpp]

  # Specialized Models
  wizardlm2:
    family: wizardlm
    size: 8x22b
    context_length: 65536
    quantization: [q4_k_m, q5_k_m]
    capabilities: [reasoning, instruction_following]
    specialization: complex_instructions
    memory_requirement: 80gb
    recommended_runtime: [vllm]
    architecture: mixture_of_experts
    
  orca2:
    family: orca
    size: 13b
    context_length: 4096
    quantization: [q4_k_m, q5_k_m]
    capabilities: [reasoning, explanation]
    specialization: step_by_step_reasoning
    memory_requirement: 10gb
    recommended_runtime: [ollama, transformers]

  # Embedding Models
  nomic-embed:
    family: nomic
    size: 1.5b
    context_length: 8192
    capabilities: [embedding]
    specialization: semantic_search
    memory_requirement: 2gb
    recommended_runtime: [ollama, transformers]
    output_dimensions: 768
    
  bge-large:
    family: bge
    size: 335m
    context_length: 512
    capabilities: [embedding, retrieval]
    specialization: document_retrieval
    memory_requirement: 1gb
    recommended_runtime: [transformers]
    output_dimensions: 1024

# Model Aliases for Easy Reference
aliases:
  coder: "deepseek-v3.2"      # Primary code generation model (Ultimate precision)
  specialist: "qwen2.5-coder-32b" # Expert code repair and review
  smart: "llama-4-maverick"   # Architectural reasoning and complex planning
  fast: "llama-4-scout"      # Quick iterations and simple fixes
  general: llama3.1
  expert: wizardlm2
  embedder: nomic-embed