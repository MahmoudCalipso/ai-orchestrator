models:
  # Code Generation Models
  deepseek-coder:
    family: deepseek
    size: 33b
    context_length: 16384
    quantization: [q4_k_m, q5_k_m, q8_0]
    capabilities: [code, reasoning]
    specialization: programming
    memory_requirement: 24gb
    recommended_runtime: [vllm, ollama]
    
  codellama:
    family: llama
    size: 34b
    context_length: 16384
    quantization: [q4_k_m, q5_k_m]
    capabilities: [code, instruct]
    specialization: code_completion
    memory_requirement: 22gb
    recommended_runtime: [ollama, llamacpp]

  # Reasoning Models
  qwen2.5:
    family: qwen
    size: 72b
    context_length: 32768
    quantization: [q4_k_m, q5_k_m, q6_k]
    capabilities: [reasoning, math, code]
    specialization: complex_reasoning
    memory_requirement: 48gb
    recommended_runtime: [vllm, transformers]
    
  llama3.1:
    family: llama
    size: 70b
    context_length: 128000
    quantization: [q4_k_m, q5_k_m]
    capabilities: [general, reasoning, long_context]
    specialization: general_purpose
    memory_requirement: 45gb
    recommended_runtime: [vllm, ollama]

  # Fast Response Models
  mistral:
    family: mistral
    size: 7b
    context_length: 32768
    quantization: [q4_k_m, q5_k_m, q8_0]
    capabilities: [general, fast]
    specialization: quick_tasks
    memory_requirement: 6gb
    recommended_runtime: [ollama, llamacpp, vllm]
    
  phi3:
    family: phi
    size: 3.8b
    context_length: 128000
    quantization: [q4_k_m, q5_k_m]
    capabilities: [general, efficient, long_context]
    specialization: resource_efficient
    memory_requirement: 4gb
    recommended_runtime: [ollama, llamacpp]

  # Specialized Models
  wizardlm2:
    family: wizardlm
    size: 8x22b
    context_length: 65536
    quantization: [q4_k_m, q5_k_m]
    capabilities: [reasoning, instruction_following]
    specialization: complex_instructions
    memory_requirement: 80gb
    recommended_runtime: [vllm]
    architecture: mixture_of_experts
    
  orca2:
    family: orca
    size: 13b
    context_length: 4096
    quantization: [q4_k_m, q5_k_m]
    capabilities: [reasoning, explanation]
    specialization: step_by_step_reasoning
    memory_requirement: 10gb
    recommended_runtime: [ollama, transformers]

  # Embedding Models
  nomic-embed:
    family: nomic
    size: 1.5b
    context_length: 8192
    capabilities: [embedding]
    specialization: semantic_search
    memory_requirement: 2gb
    recommended_runtime: [ollama, transformers]
    output_dimensions: 768
    
  bge-large:
    family: bge
    size: 335m
    context_length: 512
    capabilities: [embedding, retrieval]
    specialization: document_retrieval
    memory_requirement: 1gb
    recommended_runtime: [transformers]
    output_dimensions: 1024

# Model Aliases for Easy Reference
aliases:
  fast: mistral
  smart: qwen2.5
  coder: deepseek-coder
  efficient: phi3
  general: llama3.1
  expert: wizardlm2
  embedder: nomic-embed